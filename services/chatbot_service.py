import time
import uuid

from sqlalchemy.orm import Session

from config.config import settings
from config.open_ai_client import open_ai_client
from repositories.vector_repository import VectorRepository
from repositories.vector_retriever import VectorRetriever
from util.ansewer_evaluator import AnswerEvaluator
from util.embedding import Embedding
from util.query_expansion import ReformulationExpansion
from util.reranker import PassThroughRanker
from util.resp_text import _resp_text
import datetime

class ChatBot:
  def __init__(self):
    self.embedding=Embedding()
    self.reformulation=ReformulationExpansion()
    self.retriever=VectorRetriever()
    self.ranker=PassThroughRanker()
    self.client=open_ai_client
    ##í‰ê°€ ëª¨ë¸ ì¶”ê°€]
    self.evaluator=AnswerEvaluator()
    self.db=VectorRepository()


  async def search(self,db:Session,query):
    #ì¿¼ë¦¬ ìµìŠ¤í…ì…˜í•˜ê³  ##ì´ê±°ëŠ” ë”°ë¡œ class ë§Œë“¤ê³ 
    expand_query= await self.reformulation.expand(query)
    ##ì¿¼ë¦¬ ì„ë² ë”© -> ì´ê±°ëŠ” ì„ë² ë”© ë‹¨ì—ì„œ
    query_vec=self.embedding.query_to_embedding(expand_query["primary_query"])
    ##ë²¡í„° ê²€ìƒ‰-> ë”°ë¡œ classë¡œ ì¼ë‹¨ ë¹¼ì
    ## ë²¡í„° ì „ì²˜ë¦¬
    query_vec=self._to_pgvector_literal(query_vec)
    ##Retriever
    retrieved_chunks = self.retriever.retrieve(db=db,query_vec=query_vec, top_k=20)
    ## rankerì²˜ë¦¬
    ranked = self.ranker.rank(expand_query["primary_query"], retrieved_chunks)
    final_contexts = ranked[:5] # ì˜ˆ: ìƒìœ„ 5ê°œë§Œ LLMì— ì „ë‹¬

    return {
      "expand_query": expand_query,
      "retrieved_chunks": retrieved_chunks,
      "contexts": final_contexts,
    }
  ## ë‹µë³€ ìƒì„±
  async def answer(self,question,contexts):
    context_text = "\n\n---\n\n".join(ch.content for ch in contexts)

    prompt = f"""
      ì•„ë˜ëŠ” ì„œìš¸ì‹œë¦½ëŒ€í•™êµ ê´€ë ¨ ê³µì§€/ì•ˆë‚´ë¬¸ ì¼ë¶€ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.
      
      ë‹µë³€ì„ ìƒì„±í•  ë•Œ ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì¤˜:
      
      1) ë¨¼ì € í•™ìƒì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ëŠ” "ëŒ€í™”í˜• ë‹µë³€"ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ì œê³µí•œë‹¤.
         - ë§íˆ¬ëŠ” ë¶€ë“œëŸ½ê³  ì•ˆë‚´í•˜ëŠ” í†¤ìœ¼ë¡œ
         - í•µì‹¬ì„ ê°„ë‹¨íˆ ë¨¼ì € ì•Œë ¤ì£¼ê¸°
      
      2) ê·¸ ì•„ë˜ì—ëŠ” ì •ë³´ë¥¼ "ì •ëˆëœ ë¦¬ìŠ¤íŠ¸ í˜•ì‹"ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ìš”ì•½í•´ ì¤€ë‹¤.
         - 'ğŸ“Œ ì œëª©' í˜•íƒœì˜ ì†Œì œëª© ì‚¬ìš©
         - ê° í•­ëª©ì€ "- ë‚´ìš©" í˜•íƒœë¡œ ì¶œë ¥
         - ë‚ ì§œ/ì‹œê°„/ë°©ë²•/ì£¼ì˜ì‚¬í•­ ë“±ì„ ë³´ê¸° ì¢‹ê²Œ ì¤„ë°”ê¿ˆí•´ì„œ ì •ë¦¬
         - Markdown í—¤ë”©(#, ## ë“±)ì€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ
      
      3) ë¬¸ë‹¨ê³¼ í•­ëª© ì‚¬ì´ì—ëŠ” ë°˜ë“œì‹œ "\\n\\n" ì„ ì¶œë ¥í•´ ë¬¸ë‹¨ì„ êµ¬ë¶„í•  ê²ƒ.
         - ì ˆëŒ€ë¡œ í•œ ì¤„ì— ì—¬ëŸ¬ ì •ë³´ë¥¼ ì´ì–´ë¶™ì´ì§€ ë§ ê²ƒ
         - ê° í•­ëª©ë„ ë°˜ë“œì‹œ ìƒˆ ì¤„ì—ì„œ ì‹œì‘í•  ê²ƒ
      
      4) ì „ì²´ ë‹µë³€ì€ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì¶œë ¥í•œë‹¤.
         (JSON êµ¬ì¡°ë‚˜ ì½”ë“œë¸”ë¡ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ)
      
      5) ë„ˆë¬´ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ê³ , í‘œë‚˜ ê³µì§€ì‚¬í•­ì²˜ëŸ¼ í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½í•œë‹¤.

      ---
      [ì»¨í…ìŠ¤íŠ¸]
      {context_text}

      [ì§ˆë¬¸]
      {question}
      """

    response = self.client.responses.create(
        model=settings.OPENAI_MODEL,
        input=prompt,
    )

    answer = _resp_text(response).strip()

    return {
      "answer": answer,
      "contexts": [ch.content for ch in contexts],
    }
  ## ë³€í™˜ í•¨ìˆ˜
  @staticmethod
  def _to_pgvector_literal(vec) -> str:
    """
    [0.1, 0.2, 0.3] â†’ "[0.1,0.2,0.3]"
    pgvectorê°€ ì¸ì‹ ê°€ëŠ¥í•œ ë¬¸ìì—´ ë¦¬í„°ëŸ´ë¡œ ë³€í™˜
    """
    # numpy â†’ list ë³€í™˜
    if hasattr(vec, "tolist"):
      vec = vec.tolist()

    # floatìœ¼ë¡œ ìºìŠ¤íŒ… + ë¬¸ìì—´ ì¡°í•©
    return "[" + ",".join(str(float(x)) for x in vec) + "]"
  ##ìµœì¢… ì±„íŒ…ìš© í•¨ìˆ˜
  async def chat(self, db: Session, query: str) -> dict:

    """
    ì „ì²´ í”Œë¡œìš°:
      1) search()ë¡œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
      2) íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ 'ì•„ì˜ˆ ëª» ì°¾ì€ ê²½ìš°' early exit
      3) answer()ë¡œ 1ì°¨ ë‹µë³€ ìƒì„±
      4) AnswerEvaluatorë¡œ GOOD/BAD íŒë‹¨
    """
    # ìš”ì²­ ë‹¨ìœ„ ID, íƒ€ì„ìŠ¤íƒ¬í”„
    now = datetime.datetime.now(datetime.timezone.utc)
    request_id = f"rag-{now.strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:8]}"
    timestamp_iso = now.isoformat()

    overall_start = time.perf_counter()

    # 1) ê²€ìƒ‰
    search_result = await self.search(db, query)

    ##ë¡œê·¸ìš©
    expand_query = search_result["expand_query"]
    retrieved_chunks = search_result["retrieved_chunks"]
    contexts = search_result["contexts"]


    # context_used: ì‹¤ì œ LLMì— ë„˜ê¸´ ìƒìœ„ nê°œ
    context_used = []
    for ch in contexts:
      url = self.db.get_document_url(db, ch.doc_id)
      context_used.append({
        "doc_id": str(ch.doc_id),
        "chunk_id": str(ch.chunk_id),
        "score": float(getattr(ch, "score", 0.0)),
        "url": url,
        "text_preview": ch.content,
      })

    #ê´€ë ¨ URLë“¤ (ì¤‘ë³µ ì œê±°)
    related_urls = sorted({
      item["url"]
      for item in context_used
      if item.get("url")  # None / ë¹ˆ ë¬¸ìì—´ ì œì™¸
    })

    # --- ë¨¼ì € base_log ìƒì„± (generation/evaluationì€ ë‚˜ì¤‘ì— ì±„ì›€)
    base_log = self._build_base_log(
      request_id=request_id,
      timestamp=timestamp_iso,
      top_k=len(retrieved_chunks),
      raw_query=query,
      rewritten_query=expand_query.get("primary_query"),
      retrieval_results=context_used,
    )

    # íœ´ë¦¬ìŠ¤í‹±: ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì ìœ¼ë©´ ë°”ë¡œ "ëª¨ë¥´ê² ë‹¤" ì²˜ë¦¬
    if not contexts:
      final_answer = (
        "ê´€ë ¨ ê³µì§€ë¥¼ ì°¾ì§€ ëª»í•´ì„œ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. "
        "í•™êµ ê³µì‹ í™ˆí˜ì´ì§€ ê³µì§€ì‚¬í•­ì„ í•¨ê»˜ í™•ì¸í•´ ì£¼ì„¸ìš”."
      )

      total_latency_ms = int((time.perf_counter() - overall_start) * 1000)

      base_log["generation"] = {
        "first_token_latency_ms": total_latency_ms,  # ë¹„ìŠ¤íŠ¸ë¦¬ë°ì´ë¼ ì „ì²´ì™€ ë™ì¼í•˜ê²Œ ê¸°ë¡
        "total_latency_ms": total_latency_ms,
        "final_answer": final_answer,
      }
      base_log["evaluation"] = {
        "verdict": "NO_CONTEXT",
        "score": 0.0,
        "detail": "no contexts retrieved",
      }

      self.db.save_rag_log(db,base_log)

      return {
        "answer": final_answer,
        "contexts": [],
        "eval": base_log["evaluation"],
        "meta": {
          "reason": "NO_CONTEXT",
          "heuristic": True,
          "related_urls": related_urls,
        },
      }

    # íœ´ë¦¬ìŠ¤í‹±: score ê¸°ë°˜ ìµœì†Œ ì‹ ë¢°ë„ ì²´í¬
    # VectorRetriever.RetrievedChunkì— score í•„ë“œ ìˆë‹¤ê³  ê°€ì •
    scores = [
      getattr(ch, "score", None) for ch in contexts
      if getattr(ch, "score", None) is not None
    ]
    max_score = max(scores) if scores else 0.0

    # ì˜ˆì‹œ: max_scoreê°€ 0.4 ì•„ë˜ë©´ "ê´€ë ¨ì„± ë‚®ìŒ"ìœ¼ë¡œ ë³´ê³  ê·¸ëƒ¥ ë³´ìˆ˜ì ì¸ ë‹µë³€
    if max_score < 0.4:
      final_answer = (
        "ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ê³µì§€ë¥¼ ì¶©ë¶„íˆ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
        "ì •í™•í•œ ë‚´ìš©ì€ í•™êµ ê³µì‹ ê³µì§€ì‚¬í•­ì„ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”."
      )

      total_latency_ms = int((time.perf_counter() - overall_start) * 1000)

      base_log["generation"] = {
        "first_token_latency_ms": total_latency_ms,
        "total_latency_ms": total_latency_ms,
        "final_answer": final_answer,
      }
      base_log["evaluation"] = {
        "verdict": "LOW_RETRIEVAL_SCORE",
        "score": float(max_score),
        "detail": "max_score below threshold 0.4",
      }

      self.db.save_rag_log(db,base_log)

      return {
        "answer": final_answer,
        "contexts": [ch.content for ch in contexts],
        "eval": base_log["evaluation"],
        "meta": {
          "reason": "LOW_RETRIEVAL_SCORE",
          "max_score": max_score,
          "heuristic": True,
          "related_urls": related_urls,
        },
      }

    # 2) 1ì°¨ ë‹µë³€ ìƒì„±
    ##ì‹œê°„ ì¸¡ì •ìš©
    llm_start = time.perf_counter()
    answer_payload = await self.answer(question=query, contexts=contexts)
    ##ì‹œê°„ ì¸¡ì •ìš©
    llm_end = time.perf_counter()
    answer_text = answer_payload["answer"]

    # 3) evaluatorìš© context summary (ì§€ê¸ˆì€ ê·¸ëƒ¥ contentë¥¼ ì§§ê²Œ ì˜ë¼ì„œ ì‚¬ìš©)
    #   - ë‚˜ì¤‘ì— chunkì— eval_summary í•„ë“œ ë§Œë“¤ë©´ ê·¸ê±¸ ì“°ë©´ ë¨
    eval_context_summaries = [
      ##ì´í›„ ìš”ì•½ë³¸ ë§Œë“¤ì–´ì„œ ìš”ì•½ë³¸ ì‚¬ìš©í•˜ê¸°
      ch.content[:400]  # ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ë¬¸ì ê¸°ì¤€ ëŒ€ëµ)
      for ch in contexts[:3]  # ìƒìœ„ 3ê°œë§Œ í‰ê°€ì— ì‚¬ìš©
    ]

    eval_result = self.evaluator.evaluate(
      question=query,
      answer=answer_text,
      context_summaries=eval_context_summaries,
    )

    # 4) verdict ë”°ë¼ í›„ì²˜ë¦¬ (ì§€ê¸ˆì€ BADì—¬ë„ ì¼ë‹¨ ë‹µë³€ì€ ì£¼ë˜, ë©”íƒ€ì— í‘œì‹œ)
    verdict = eval_result.get("verdict", "BAD")

    # BADì¼ ë•Œ ê²½ê³  ë¬¸êµ¬ ì‚´ì§ ë¶™ì—¬ì£¼ê¸° (ì„ íƒì‚¬í•­)
    if verdict == "BAD":
      safe_answer = (
        answer_text
        + " "
        + "(âš ï¸ ì´ ë‹µë³€ì€ ì œê³µëœ ê³µì§€ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ë‚´ìš©ì€ ë°˜ë“œì‹œ ê³µì‹ ê³µì§€ ì›ë¬¸ì„ í•¨ê»˜ í™•ì¸í•´ ì£¼ì„¸ìš”.)"
      )
      answer_payload["answer"] = safe_answer
    # 6) generation/evaluation ë¡œê·¸ ì±„ìš°ê¸°
    total_latency_ms = int((time.perf_counter() - overall_start) * 1000)
    first_token_latency_ms = int((llm_end - llm_start) * 1000)
    # ìµœì¢… ë°˜í™˜ êµ¬ì¡°: answer + contexts + eval ë©”íƒ€
    base_log["generation"] = {
      "first_token_latency_ms": first_token_latency_ms,
      "total_latency_ms": total_latency_ms,
      "final_answer": answer_text,
    }
    base_log["evaluation"] = eval_result

    self.db.save_rag_log(db, base_log)

    return {
      "answer": answer_payload["answer"],
      "contexts": answer_payload["contexts"],
      "eval": eval_result,
      "meta": {
        "max_retrieval_score": max_score,
        "used_heuristic": False,  # ìœ„ì—ì„œ early-return í•œ ì¼€ì´ìŠ¤ë§Œ True
        "verdict": verdict,
        "related_urls": related_urls,
      },
    }

  def _build_base_log(
      self,
      *,
      request_id: str,
      timestamp: str,
      top_k: int,
      raw_query: str,
      rewritten_query: str | None,
      retrieval_results: list[dict],
  ) -> dict:
    return {
      "metadata": {
        "request_id": request_id,
        "timestamp": timestamp,
        "retrival_top_k": top_k,
      },
      "query": {
        "raw": raw_query,
        "rewritten": rewritten_query,
      },
      "retrieval_used": {
        "results": retrieval_results,
      },
      # "generation": ...   # ì•„ë˜ì—ì„œ ì±„ì›€
      # "evaluation": ...   # ì•„ë˜ì—ì„œ ì±„ì›€
    }

